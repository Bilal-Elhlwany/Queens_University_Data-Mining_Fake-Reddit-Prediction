# Queens_University_Data-Mining_Fake-Reddit-Prediction
**Reddit Fake Post Detection (by Looking Only at the Title)**

![inbox_4409738_4f5fda2da319a6815c68c937f7474d4c_mememe_3292829e1a7b6e4f95de73cc5e318807-1](https://github.com/Bilal-Elhlwany/Queens_University_Data-Mining_Fake-Reddit-Prediction/assets/100938358/127e6fa7-91e3-45b7-bdcc-cec4002b11e6)

False information on the Internet has caused many social problems due to the raise of social network and its role in different domains such as politics. In this assignment, we are going to predict if a specific reddit post is fake news or not, by looking at its title. The data is raw (contains various forms of words) so it is up to you on what text preprocessing techniques to be applied.
https://www.kaggle.com/competitions/cisc-873-dm-w23-a3/overview

Enjoy!

**Steps:**

**‚úîÔ∏è Meme competition [optional]:**

Include/find a MEME that you liked related to data science/data mining/machine learning. You can upload yours here.

**‚úîÔ∏è Problem Formulation:**

Define the problem. What is the input? What is the output? What data mining function is required? What could be the challenges? What is the impact? What is an ideal solution?
(You can put your answers in markdown format in your notebook - Text Cell)

**‚úîÔ∏è Document your code:**

Put detailed comment on each line of the code to show your understanding of the your code. Also describe: What is the experimental protocol used and how was it carried out? What preprocessing steps are used?
(You can put your answers in markdown format in your notebook - Text Cell)

**‚úîÔ∏è Model Tuning and Documentation:**

Now based on the template, try to improve the model's performance on the public leaderboard by following the data science life-cycle for tuning. You can try different features, different hyperparameters/configurations of the model, and even a different model. For each trial, document the reason why you want to make the certain change and the expected outcome, before running the code. Record the observed performance and your thought on it. The final result is not important, but the process is. Documentation of your thought process is very important, since most people forgot why they test certain model/hyperparameter after they got the result (it takes time). It also helps a lot when you got stuck. You can organize the notebook by listing

thoughts and observations for trial 0, plan for trial 1

code for trial 1

thoughts and observations for trial 1, plan for trial 2

code for trial 2
‚Ä¶
You have to tune at least 5 times. All the tried solutions should be different (e.g. different feature sets/different preprocessing/model/tuning method/tuning range). Requirements - covered at least:

At least two different text preprocessing techniques (case normalization does not count)

A tunable pipeline including the vectorizer.

Cover both character-level vectorizer and word-level vectorizer.

At least one hyperparamter search method (grid/random/bayesian) with validation set (not cross-validation).

At least cover xgboosting.

**‚úîÔ∏è Answer the questions below (briefly):**

**üåà What is the difference between Character n-gram and Word n-gram? Which one tends to suffer more from the OOV issue?**

**üåà What is the difference between stop word removal and stemming? Are these techniques language-dependent?**

**üåà Is tokenization techniques language dependent? Why?**

**üåà What is the difference between count vectorizer and tf-idf vectorizer? Would it be feasible to use all possible n-grams? If not, how should you select them?**

Deliverable:

üî• A single python notebook containing the documentation of how you reach the final design as well as the answers to the questions.


Note: Emoji allowed and encouraged so it will be more fun for us to grade it.

There are a lot of resources available online, especially for medium.com. For posts that requires tokens/credits, you can open and read that page in incognito mode of your browser.
